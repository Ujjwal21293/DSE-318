{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Name: Ujjwal Kishor Sahoo | Roll Number: 21293 | NLP-Assignment 2**"
      ],
      "metadata": {
        "id": "uqkRA94q5yQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3S2zHPusL71",
        "outputId": "3443df8e-b717-4f14-afe7-f728f77cafb8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the dataset**"
      ],
      "metadata": {
        "id": "WQy4KnNS6BU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/train.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/val.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/train.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/val.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/train.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/val.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMMEwDUFrY0R",
        "outputId": "d19bd67e-7594-4233-fb39-3937bb9d5291"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 10:28:34--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406615 (397K) [text/plain]\n",
            "Saving to: ‘train.csv.3’\n",
            "\n",
            "\rtrain.csv.3           0%[                    ]       0  --.-KB/s               \rtrain.csv.3         100%[===================>] 397.08K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-04 10:28:34 (8.58 MB/s) - ‘train.csv.3’ saved [406615/406615]\n",
            "\n",
            "--2025-04-04 10:28:34--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49821 (49K) [text/plain]\n",
            "Saving to: ‘val.csv.3’\n",
            "\n",
            "val.csv.3           100%[===================>]  48.65K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-04-04 10:28:35 (3.37 MB/s) - ‘val.csv.3’ saved [49821/49821]\n",
            "\n",
            "--2025-04-04 10:28:35--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 192093 (188K) [text/plain]\n",
            "Saving to: ‘train.csv.4’\n",
            "\n",
            "train.csv.4         100%[===================>] 187.59K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-04 10:28:35 (5.28 MB/s) - ‘train.csv.4’ saved [192093/192093]\n",
            "\n",
            "--2025-04-04 10:28:35--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23964 (23K) [text/plain]\n",
            "Saving to: ‘val.csv.4’\n",
            "\n",
            "val.csv.4           100%[===================>]  23.40K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-04-04 10:28:35 (9.23 MB/s) - ‘val.csv.4’ saved [23964/23964]\n",
            "\n",
            "--2025-04-04 10:28:35--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472095 (461K) [text/plain]\n",
            "Saving to: ‘train.csv.5’\n",
            "\n",
            "train.csv.5         100%[===================>] 461.03K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-04 10:28:35 (9.44 MB/s) - ‘train.csv.5’ saved [472095/472095]\n",
            "\n",
            "--2025-04-04 10:28:35--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58401 (57K) [text/plain]\n",
            "Saving to: ‘val.csv.5’\n",
            "\n",
            "val.csv.5           100%[===================>]  57.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-04-04 10:28:36 (4.31 MB/s) - ‘val.csv.5’ saved [58401/58401]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-uILIYkzDY-",
        "outputId": "4b16cb2b-a4b7-4814-81d5-6d2d0dba8758"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "14QjhxXYrVoP"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Machine learning and neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Embedding, Dense, Flatten, Input, Concatenate, Dropout)\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.metrics import Precision, Recall\n",
        "from kerastuner import HyperParameters\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from keras_tuner import Objective\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Text processing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Hyperparameter tuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner import RandomSearch, Objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "yBTH1GFgrVoR"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hate Dataset"
      ],
      "metadata": {
        "id": "1vPrjlIGEziz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "o9O1tlJtrVoR"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "hate_train_data = pd.read_csv('/content/train.csv')\n",
        "hate_test_data = pd.read_csv('/content/val.csv')\n",
        "\n",
        "# Prepare the text and labels\n",
        "texts = hate_train_data['Sentence'].values\n",
        "labels = hate_train_data['Tag'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing**"
      ],
      "metadata": {
        "id": "ZXPzhJzH6Tcl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "92QJt-xvrVoR"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization and lemmatization using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Stopword removal and lemmatization\n",
        "    text = re.sub(r\"https\\\\S+|www\\\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\\\s]\", \"\", text)  # remove special characters\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "texts_preprocessed = [preprocess_text(text) for text in texts]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_preprocessed)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts_preprocessed)\n",
        "\n",
        "max_sequence_len = 128\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "T4cJIsdprVoS"
      },
      "outputs": [],
      "source": [
        "# Preparing dataset\n",
        "sentences = [text.split() for text in texts_preprocessed]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define constants\n",
        "embedding_dim = 100\n",
        "window_size = 5  # For skip-gram context window\n",
        "\n",
        "# Build a simple skip-gram pair generator function\n",
        "def generate_skipgram_pairs(sentences, window_size, vocab_size):\n",
        "    skipgrams = []\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in tokenizer.word_index:\n",
        "                continue  # Skip unknown words\n",
        "            target_word = tokenizer.word_index[word]\n",
        "            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\n",
        "            context_words = [\n",
        "                tokenizer.word_index[w]\n",
        "                for w in context_window\n",
        "                if w != word and w in tokenizer.word_index\n",
        "            ]\n",
        "            for context_word in context_words:\n",
        "                skipgrams.append([target_word, context_word])\n",
        "    return np.array(skipgrams)\n",
        "\n",
        "skipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting target words and context words\n",
        "X_target, X_context = zip(*skipgrams)\n",
        "X_target = np.array(X_target)\n",
        "X_context = np.array(X_context)\n",
        "\n",
        "# Defining the custom Word2Vec model using Keras\n",
        "input_target = tf.keras.layers.Input(shape=(1,))\n",
        "input_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "# Embedding layer for target and context\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\n",
        "target_embedding = embedding(input_target)\n",
        "context_embedding = embedding(input_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpihCWrV34fy",
        "outputId": "1ad9a3b4-8fe1-49ab-9665-a89a0f3620f7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape embedding output for dot product calculation\n",
        "target_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\n",
        "context_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\n",
        "\n",
        "# Compute dot product (cosine similarity between target and context)\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# Define the model and compile it\n",
        "word2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
        "word2vec_model.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "EE2px0JK4Fhx"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare labels (1 for correct context, 0 for negative samples)\n",
        "labels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\n",
        "\n",
        "# Train the model\n",
        "word2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Extract the trained word embeddings\n",
        "trained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('word2vec_embeddings_hate.npy', trained_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fk8FPLl4I0U",
        "outputId": "867bd819-88d6-4859-aa3e-105b946be513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4395/4395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 10ms/step - loss: 0.2095\n",
            "Epoch 2/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo83GuOqrVoS"
      },
      "outputs": [],
      "source": [
        "# Load the custom-trained Word2Vec embeddings\n",
        "trained_embeddings = np.load('word2vec_embeddings_hate.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "towm2lPSrVoS"
      },
      "outputs": [],
      "source": [
        "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
        "labels = hate_train_data['Tag'].values\n",
        "\n",
        "# Now perform train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.1, random_state=60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdSO8ka-rVoT"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data\n",
        "test_texts = hate_test_data['Sentence'].values\n",
        "test_texts_preprocessed = [preprocess_text(text) for text in test_texts]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining custom evalutation metric**"
      ],
      "metadata": {
        "id": "P4e4YGtX6aKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgl4FvHMrVoT"
      },
      "outputs": [],
      "source": [
        "# Define a custom macro F1 score function\n",
        "def macro_f1_score(y_true, y_pred):\n",
        "    # Ensure both y_true and y_pred are float32\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Convert predictions to binary (0 or 1)\n",
        "    y_pred_bin = tf.round(y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    def f1(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "        f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "        return f1_val\n",
        "\n",
        "    f1_per_class = f1(y_true, y_pred_bin)\n",
        "    return K.mean(f1_per_class)  # Macro F1 score (mean across all classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "xG1FK0v96hW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBBspwjRrVoU"
      },
      "outputs": [],
      "source": [
        "# Function to build the FFNN model for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=sequence_len,\n",
        "        weights=[trained_embeddings],\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\n",
        "    for i in range(num_layers):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\n",
        "            activation='tanh'\n",
        "        ))\n",
        "\n",
        "    # Binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "\n",
        "    if optimizer_choice == 'Adam':\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "    elif optimizer_choice == 'AdamW':\n",
        "        opt = AdamW(learning_rate=0.001)\n",
        "    else:\n",
        "        opt = SGD(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[macro_f1_score]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the objective using KerasTuner's Objective class\n",
        "objective = Objective('val_macro_f1_score', direction='max')\n",
        "\n",
        "# Hyperparameter search with Keras Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=objective,  # Explicitly specify the objective\n",
        "    max_trials=15,  # Number of hyperparameter configurations to try\n",
        "    executions_per_trial=1,  # Number of times to train each model configuration\n",
        "    directory='hyperparam_tuning_hate',\n",
        "    project_name='ffnn_tuning_v7.2213123123'\n",
        ")\n",
        "\n",
        "# Run the tuner search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model_ffnn = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model_ffnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "E7-2_e8I6qCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvRrrAXzrVoV"
      },
      "outputs": [],
      "source": [
        "# Save FFNN Model\n",
        "best_model_ffnn.save('best_model_ffnn.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the saved models**"
      ],
      "metadata": {
        "id": "X9OFtbJn6xR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved files\n",
        "best_model_ffnn = tf.keras.models.load_model('best_model_ffnn.keras', custom_objects={'macro_f1_score': macro_f1_score})"
      ],
      "metadata": {
        "id": "btp5estysaLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW0aeQXwrVoV"
      },
      "outputs": [],
      "source": [
        "# Function to extract and print the macro avg F1-score\n",
        "def print_macro_f1(classification_report_dict, model_name):\n",
        "    macro_f1 = classification_report_dict['macro avg']['f1-score']\n",
        "    print(f\"{model_name} Model Macro Average F1-score: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "BFk0sGAZ66oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGzqtOPnrVoV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Binary labels (0 or 1)\n",
        "test_labels = hate_test_data['Tag'].values.reshape(-1, 1)\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "model = best_model_ffnn\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "reports = {}\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Generate classification report\n",
        "report = classification_report(\n",
        "test_labels,\n",
        "predictions,\n",
        "target_names=['Non-Hate (0)', 'Hate (1)'],  # Optional: change labels as you wish\n",
        "output_dict=True)\n",
        "\n",
        "reports[model] = report\n",
        "\n",
        "# Print macro F1 score and optionally others\n",
        "f1 = f1_score(test_labels, predictions, average='macro')\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"{model} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8MZz9Ib6u-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Humor Dataset"
      ],
      "metadata": {
        "id": "sRKnKiRxF_Zg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aswkBEg5GEJi"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "hate_train_data = pd.read_csv('/content/train.csv.1')\n",
        "hate_test_data = pd.read_csv('/content/val.csv.1')\n",
        "\n",
        "# Prepare the text and labels\n",
        "texts = hate_train_data['Sentence'].values\n",
        "labels = hate_train_data['Tag'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing**"
      ],
      "metadata": {
        "id": "c0RJyg0vGEJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVclC5_3GEJj"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization and lemmatization using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Stopword removal and lemmatization\n",
        "    text = re.sub(r\"https\\\\S+|www\\\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\\\s]\", \"\", text)  # remove special characters\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "texts_preprocessed = [preprocess_text(text) for text in texts]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_preprocessed)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts_preprocessed)\n",
        "\n",
        "max_sequence_len = 128\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_VfMuGoGEJk"
      },
      "outputs": [],
      "source": [
        "# Preparing dataset\n",
        "sentences = [text.split() for text in texts_preprocessed]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define constants\n",
        "embedding_dim = 100\n",
        "window_size = 5  # For skip-gram context window\n",
        "\n",
        "# Build a simple skip-gram pair generator function\n",
        "def generate_skipgram_pairs(sentences, window_size, vocab_size):\n",
        "    skipgrams = []\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in tokenizer.word_index:\n",
        "                continue  # Skip unknown words\n",
        "            target_word = tokenizer.word_index[word]\n",
        "            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\n",
        "            context_words = [\n",
        "                tokenizer.word_index[w]\n",
        "                for w in context_window\n",
        "                if w != word and w in tokenizer.word_index\n",
        "            ]\n",
        "            for context_word in context_words:\n",
        "                skipgrams.append([target_word, context_word])\n",
        "    return np.array(skipgrams)\n",
        "\n",
        "skipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting target words and context words\n",
        "X_target, X_context = zip(*skipgrams)\n",
        "X_target = np.array(X_target)\n",
        "X_context = np.array(X_context)\n",
        "\n",
        "# Defining the custom Word2Vec model using Keras\n",
        "input_target = tf.keras.layers.Input(shape=(1,))\n",
        "input_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "# Embedding layer for target and context\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\n",
        "target_embedding = embedding(input_target)\n",
        "context_embedding = embedding(input_context)"
      ],
      "metadata": {
        "id": "mkmcgNMGGEJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape embedding output for dot product calculation\n",
        "target_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\n",
        "context_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\n",
        "\n",
        "# Compute dot product (cosine similarity between target and context)\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# Define the model and compile it\n",
        "word2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
        "word2vec_model.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "L80pHT2yGEJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare labels (1 for correct context, 0 for negative samples)\n",
        "labels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\n",
        "\n",
        "# Train the model\n",
        "word2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Extract the trained word embeddings\n",
        "trained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('word2vec_embeddings_humor.npy', trained_embeddings)"
      ],
      "metadata": {
        "id": "6F6bsvFDGEJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcB8j542GEJn"
      },
      "outputs": [],
      "source": [
        "# Load the custom-trained Word2Vec embeddings\n",
        "trained_embeddings = np.load('word2vec_embeddings_humor.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmAM6bwiGEJn"
      },
      "outputs": [],
      "source": [
        "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
        "labels = hate_train_data['Tag'].values\n",
        "\n",
        "# Now perform train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.1, random_state=60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hACm0p6QGEJn"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data\n",
        "test_texts = hate_test_data['Sentence'].values\n",
        "test_texts_preprocessed = [preprocess_text(text) for text in test_texts]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining custom evalutation metric**"
      ],
      "metadata": {
        "id": "aTomMju-GEJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-I7gzGyGEJo"
      },
      "outputs": [],
      "source": [
        "# Define a custom macro F1 score function\n",
        "def macro_f1_score(y_true, y_pred):\n",
        "    # Ensure both y_true and y_pred are float32\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Convert predictions to binary (0 or 1)\n",
        "    y_pred_bin = tf.round(y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    def f1(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "        f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "        return f1_val\n",
        "\n",
        "    f1_per_class = f1(y_true, y_pred_bin)\n",
        "    return K.mean(f1_per_class)  # Macro F1 score (mean across all classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "e3TWLQhzGEJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j5RYc2YGEJo"
      },
      "outputs": [],
      "source": [
        "# Function to build the FFNN model for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=sequence_len,\n",
        "        weights=[trained_embeddings],\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\n",
        "    for i in range(num_layers):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\n",
        "            activation='tanh'\n",
        "        ))\n",
        "\n",
        "    # Binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "\n",
        "    if optimizer_choice == 'Adam':\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "    elif optimizer_choice == 'AdamW':\n",
        "        opt = AdamW(learning_rate=0.001)\n",
        "    else:\n",
        "        opt = SGD(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[macro_f1_score]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the objective using KerasTuner's Objective class\n",
        "objective = Objective('val_macro_f1_score', direction='max')\n",
        "\n",
        "# Hyperparameter search with Keras Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=objective,  # Explicitly specify the objective\n",
        "    max_trials=15,  # Number of hyperparameter configurations to try\n",
        "    executions_per_trial=1,  # Number of times to train each model configuration\n",
        "    directory='hyperparam_tuning_humor',\n",
        "    project_name='ffnn_tuning_v7.2213123123'\n",
        ")\n",
        "\n",
        "# Run the tuner search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model_ffnn = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model_ffnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "na_P4mPsGEJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41HcVShwGEJp"
      },
      "outputs": [],
      "source": [
        "# Save FFNN Model\n",
        "best_model_ffnn.save('best_model_ffnn.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the saved models**"
      ],
      "metadata": {
        "id": "JItgJIIdGEJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved files\n",
        "best_model_ffnn = tf.keras.models.load_model('best_model_ffnn.keras', custom_objects={'macro_f1_score': macro_f1_score})"
      ],
      "metadata": {
        "id": "pHctoPgcGEJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImnPC3Z3GEJp"
      },
      "outputs": [],
      "source": [
        "# Function to extract and print the macro avg F1-score\n",
        "def print_macro_f1(classification_report_dict, model_name):\n",
        "    macro_f1 = classification_report_dict['macro avg']['f1-score']\n",
        "    print(f\"{model_name} Model Macro Average F1-score: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "7vbn8YCRGEJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmzRBX1TGEJq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Binary labels (0 or 1)\n",
        "test_labels = hate_test_data['Tag'].values.reshape(-1, 1)\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "model = best_model_ffnn\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "reports = {}\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Generate classification report\n",
        "report = classification_report(\n",
        "test_labels,\n",
        "predictions,\n",
        "target_names=['Non-Hate (0)', 'Hate (1)'],  # Optional: change labels as you wish\n",
        "output_dict=True)\n",
        "\n",
        "reports[model] = report\n",
        "\n",
        "# Print macro F1 score and optionally others\n",
        "f1 = f1_score(test_labels, predictions, average='macro')\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"{model} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jc2eDTg_GRwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarcasm Dataset"
      ],
      "metadata": {
        "id": "vCxf1YIJGTZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EozSQAW3GlUu"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "hate_train_data = pd.read_csv('/content/train.csv.2')\n",
        "hate_test_data = pd.read_csv('/content/val.csv.2')\n",
        "\n",
        "# Prepare the text and labels\n",
        "texts = hate_train_data['Sentence'].values\n",
        "labels = hate_train_data['Tag'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing**"
      ],
      "metadata": {
        "id": "-uFkLbwNGlUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJxJ1KH9GlUw"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization and lemmatization using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Stopword removal and lemmatization\n",
        "    text = re.sub(r\"https\\\\S+|www\\\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\\\s]\", \"\", text)  # remove special characters\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "texts_preprocessed = [preprocess_text(text) for text in texts]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_preprocessed)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts_preprocessed)\n",
        "\n",
        "max_sequence_len = 128\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnOzwBDmGlUw"
      },
      "outputs": [],
      "source": [
        "# Preparing dataset\n",
        "sentences = [text.split() for text in texts_preprocessed]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define constants\n",
        "embedding_dim = 100\n",
        "window_size = 5  # For skip-gram context window\n",
        "\n",
        "# Build a simple skip-gram pair generator function\n",
        "def generate_skipgram_pairs(sentences, window_size, vocab_size):\n",
        "    skipgrams = []\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in tokenizer.word_index:\n",
        "                continue  # Skip unknown words\n",
        "            target_word = tokenizer.word_index[word]\n",
        "            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\n",
        "            context_words = [\n",
        "                tokenizer.word_index[w]\n",
        "                for w in context_window\n",
        "                if w != word and w in tokenizer.word_index\n",
        "            ]\n",
        "            for context_word in context_words:\n",
        "                skipgrams.append([target_word, context_word])\n",
        "    return np.array(skipgrams)\n",
        "\n",
        "skipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting target words and context words\n",
        "X_target, X_context = zip(*skipgrams)\n",
        "X_target = np.array(X_target)\n",
        "X_context = np.array(X_context)\n",
        "\n",
        "# Defining the custom Word2Vec model using Keras\n",
        "input_target = tf.keras.layers.Input(shape=(1,))\n",
        "input_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "# Embedding layer for target and context\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\n",
        "target_embedding = embedding(input_target)\n",
        "context_embedding = embedding(input_context)"
      ],
      "metadata": {
        "id": "kFZrgK9fGlUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape embedding output for dot product calculation\n",
        "target_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\n",
        "context_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\n",
        "\n",
        "# Compute dot product (cosine similarity between target and context)\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# Define the model and compile it\n",
        "word2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
        "word2vec_model.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "gHWj4vJ8GlUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare labels (1 for correct context, 0 for negative samples)\n",
        "labels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\n",
        "\n",
        "# Train the model\n",
        "word2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Extract the trained word embeddings\n",
        "trained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('word2vec_embeddings_sarcasm.npy', trained_embeddings)"
      ],
      "metadata": {
        "id": "DYXO0uaoGlUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY62Cd4OGlUy"
      },
      "outputs": [],
      "source": [
        "# Load the custom-trained Word2Vec embeddings\n",
        "trained_embeddings = np.load('word2vec_embeddings_sarcasm.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUbjVvd7GlUy"
      },
      "outputs": [],
      "source": [
        "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
        "labels = hate_train_data['Tag'].values\n",
        "\n",
        "# Now perform train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.1, random_state=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjpZCGi-GlUz"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data\n",
        "test_texts = hate_test_data['Sentence'].values\n",
        "test_texts_preprocessed = [preprocess_text(text) for text in test_texts]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining custom evalutation metric**"
      ],
      "metadata": {
        "id": "uJPsLVKRGlUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53k9BeyEGlUz"
      },
      "outputs": [],
      "source": [
        "# Define a custom macro F1 score function\n",
        "def macro_f1_score(y_true, y_pred):\n",
        "    # Ensure both y_true and y_pred are float32\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Convert predictions to binary (0 or 1)\n",
        "    y_pred_bin = tf.round(y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    def f1(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "        f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "        return f1_val\n",
        "\n",
        "    f1_per_class = f1(y_true, y_pred_bin)\n",
        "    return K.mean(f1_per_class)  # Macro F1 score (mean across all classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "Wn5ItCOrGlUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPPjXpXxGlU0"
      },
      "outputs": [],
      "source": [
        "# Function to build the FFNN model for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=sequence_len,\n",
        "        weights=[trained_embeddings],\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\n",
        "    for i in range(num_layers):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\n",
        "            activation='tanh'\n",
        "        ))\n",
        "\n",
        "    # Binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "\n",
        "    if optimizer_choice == 'Adam':\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "    elif optimizer_choice == 'AdamW':\n",
        "        opt = AdamW(learning_rate=0.001)\n",
        "    else:\n",
        "        opt = SGD(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[macro_f1_score]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the objective using KerasTuner's Objective class\n",
        "objective = Objective('val_macro_f1_score', direction='max')\n",
        "\n",
        "# Hyperparameter search with Keras Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=objective,  # Explicitly specify the objective\n",
        "    max_trials=15,  # Number of hyperparameter configurations to try\n",
        "    executions_per_trial=1,  # Number of times to train each model configuration\n",
        "    directory='hyperparam_tuning_sarcasm',\n",
        "    project_name='ffnn_tuning_v7.2213123123'\n",
        ")\n",
        "\n",
        "# Run the tuner search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model_ffnn = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model_ffnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "xe0kWgDEGlU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBeWZ2D1GlU0"
      },
      "outputs": [],
      "source": [
        "# Save FFNN Model\n",
        "best_model_ffnn.save('best_model_ffnn.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the saved models**"
      ],
      "metadata": {
        "id": "dPjsF4HyGlU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved files\n",
        "best_model_ffnn = tf.keras.models.load_model('best_model_ffnn.keras', custom_objects={'macro_f1_score': macro_f1_score})"
      ],
      "metadata": {
        "id": "MN0YloI3GlU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjXOn6OsGlU1"
      },
      "outputs": [],
      "source": [
        "# Function to extract and print the macro avg F1-score\n",
        "def print_macro_f1(classification_report_dict, model_name):\n",
        "    macro_f1 = classification_report_dict['macro avg']['f1-score']\n",
        "    print(f\"{model_name} Model Macro Average F1-score: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "76NwrGrXGlU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS5Tsf48GlU1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Binary labels (0 or 1)\n",
        "test_labels = hate_test_data['Tag'].values.reshape(-1, 1)\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "model = best_model_ffnn\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "reports = {}\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Generate classification report\n",
        "report = classification_report(\n",
        "test_labels,\n",
        "predictions,\n",
        "target_names=['Non-Hate (0)', 'Hate (1)'],  # Optional: change labels as you wish\n",
        "output_dict=True)\n",
        "\n",
        "reports[model] = report\n",
        "\n",
        "# Print macro F1 score and optionally others\n",
        "f1 = f1_score(test_labels, predictions, average='macro')\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"{model} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1Hf186ZVKE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}