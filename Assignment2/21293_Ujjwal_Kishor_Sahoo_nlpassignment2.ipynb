{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Name: Ujjwal Kishor Sahoo | Roll Number: 21293 | NLP-Assignment 2**"
      ],
      "metadata": {
        "id": "uqkRA94q5yQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3S2zHPusL71",
        "outputId": "8722a1b5-5f46-403c-9f39-277523da33f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the dataset**"
      ],
      "metadata": {
        "id": "WQy4KnNS6BU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/train.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/val.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/train.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/val.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/train.csv\n",
        "!wget https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/val.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMMEwDUFrY0R",
        "outputId": "515562e9-09a2-4399-c6ae-daa7f720f14d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 15:31:34--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406615 (397K) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 397.08K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-04 15:31:35 (13.5 MB/s) - ‘train.csv’ saved [406615/406615]\n",
            "\n",
            "--2025-04-04 15:31:35--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/hate/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49821 (49K) [text/plain]\n",
            "Saving to: ‘val.csv’\n",
            "\n",
            "val.csv             100%[===================>]  48.65K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-04-04 15:31:35 (5.53 MB/s) - ‘val.csv’ saved [49821/49821]\n",
            "\n",
            "--2025-04-04 15:31:35--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 192093 (188K) [text/plain]\n",
            "Saving to: ‘train.csv.1’\n",
            "\n",
            "train.csv.1         100%[===================>] 187.59K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-04-04 15:31:36 (8.07 MB/s) - ‘train.csv.1’ saved [192093/192093]\n",
            "\n",
            "--2025-04-04 15:31:36--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/humor/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23964 (23K) [text/plain]\n",
            "Saving to: ‘val.csv.1’\n",
            "\n",
            "val.csv.1           100%[===================>]  23.40K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-04-04 15:31:36 (13.5 MB/s) - ‘val.csv.1’ saved [23964/23964]\n",
            "\n",
            "--2025-04-04 15:31:36--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472095 (461K) [text/plain]\n",
            "Saving to: ‘train.csv.2’\n",
            "\n",
            "train.csv.2         100%[===================>] 461.03K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-04 15:31:36 (13.9 MB/s) - ‘train.csv.2’ saved [472095/472095]\n",
            "\n",
            "--2025-04-04 15:31:36--  https://raw.githubusercontent.com/islnlp/Assignment_1_2025/refs/heads/main/sarcasm/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58401 (57K) [text/plain]\n",
            "Saving to: ‘val.csv.2’\n",
            "\n",
            "val.csv.2           100%[===================>]  57.03K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-04-04 15:31:37 (8.03 MB/s) - ‘val.csv.2’ saved [58401/58401]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-uILIYkzDY-",
        "outputId": "e5fdd205-f6e7-4280-94af-e443739c7667"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "14QjhxXYrVoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c7ae17-5753-43e2-fdd7-609bf302853b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c829fb601b69>:18: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperParameters\n"
          ]
        }
      ],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Machine learning and neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Embedding, Dense, Flatten, Input, Concatenate, Dropout)\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.metrics import Precision, Recall\n",
        "from kerastuner import HyperParameters\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from keras_tuner import Objective\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Text processing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Hyperparameter tuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner import RandomSearch, Objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yBTH1GFgrVoR"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hate Dataset"
      ],
      "metadata": {
        "id": "1vPrjlIGEziz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o9O1tlJtrVoR"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "hate_train_data = pd.read_csv('/content/train.csv')\n",
        "hate_test_data = pd.read_csv('/content/val.csv')\n",
        "\n",
        "# Prepare the text and labels\n",
        "texts = hate_train_data['Sentence'].values\n",
        "labels = hate_train_data['Tag'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing and Embedding**"
      ],
      "metadata": {
        "id": "ZXPzhJzH6Tcl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "92QJt-xvrVoR"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization and lemmatization using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Stopword removal and lemmatization\n",
        "    text = re.sub(r\"https\\\\S+|www\\\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\\\s]\", \"\", text)  # remove special characters\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "texts_preprocessed = [preprocess_text(text) for text in texts]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_preprocessed)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts_preprocessed)\n",
        "\n",
        "max_sequence_len = 128\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T4cJIsdprVoS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "52c291e7-7521-4ec2-d70a-924989730f4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Preparing dataset\\nsentences = [text.split() for text in texts_preprocessed]\\nvocab_size = len(tokenizer.word_index) + 1\\n\\n# Define constants\\nembedding_dim = 100\\nwindow_size = 5  # For skip-gram context window\\n\\n# Build a simple skip-gram pair generator function\\ndef generate_skipgram_pairs(sentences, window_size, vocab_size):\\n    skipgrams = []\\n    for sentence in sentences:\\n        for i, word in enumerate(sentence):\\n            if word not in tokenizer.word_index:\\n                continue  # Skip unknown words\\n            target_word = tokenizer.word_index[word]\\n            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\\n            context_words = [\\n                tokenizer.word_index[w]\\n                for w in context_window\\n                if w != word and w in tokenizer.word_index\\n            ]\\n            for context_word in context_words:\\n                skipgrams.append([target_word, context_word])\\n    return np.array(skipgrams)\\n\\nskipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''# Preparing dataset\n",
        "sentences = [text.split() for text in texts_preprocessed]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define constants\n",
        "embedding_dim = 100\n",
        "window_size = 5  # For skip-gram context window\n",
        "\n",
        "# Build a simple skip-gram pair generator function\n",
        "def generate_skipgram_pairs(sentences, window_size, vocab_size):\n",
        "    skipgrams = []\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in tokenizer.word_index:\n",
        "                continue  # Skip unknown words\n",
        "            target_word = tokenizer.word_index[word]\n",
        "            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\n",
        "            context_words = [\n",
        "                tokenizer.word_index[w]\n",
        "                for w in context_window\n",
        "                if w != word and w in tokenizer.word_index\n",
        "            ]\n",
        "            for context_word in context_words:\n",
        "                skipgrams.append([target_word, context_word])\n",
        "    return np.array(skipgrams)\n",
        "\n",
        "skipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Splitting target words and context words\n",
        "X_target, X_context = zip(*skipgrams)\n",
        "X_target = np.array(X_target)\n",
        "X_context = np.array(X_context)\n",
        "\n",
        "# Defining the custom Word2Vec model using Keras\n",
        "input_target = tf.keras.layers.Input(shape=(1,))\n",
        "input_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "# Embedding layer for target and context\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\n",
        "target_embedding = embedding(input_target)\n",
        "context_embedding = embedding(input_context)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "fpihCWrV34fy",
        "outputId": "4af0e6e9-cba6-4251-ea26-53ae3aafa4a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Splitting target words and context words\\nX_target, X_context = zip(*skipgrams)\\nX_target = np.array(X_target)\\nX_context = np.array(X_context)\\n\\n# Defining the custom Word2Vec model using Keras\\ninput_target = tf.keras.layers.Input(shape=(1,))\\ninput_context = tf.keras.layers.Input(shape=(1,))\\n\\n# Embedding layer for target and context\\nembedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\\ntarget_embedding = embedding(input_target)\\ncontext_embedding = embedding(input_context)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Reshape embedding output for dot product calculation\n",
        "target_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\n",
        "context_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\n",
        "\n",
        "# Compute dot product (cosine similarity between target and context)\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# Define the model and compile it\n",
        "word2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
        "word2vec_model.compile(optimizer='adam', loss='binary_crossentropy')'''"
      ],
      "metadata": {
        "id": "EE2px0JK4Fhx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "c8f21186-391d-4660-996c-f450807b234d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Reshape embedding output for dot product calculation\\ntarget_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\\ncontext_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\\n\\n# Compute dot product (cosine similarity between target and context)\\ndot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\\n\\n# Define the model and compile it\\nword2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\\nword2vec_model.compile(optimizer='adam', loss='binary_crossentropy')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Prepare labels (1 for correct context, 0 for negative samples)\n",
        "labels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\n",
        "\n",
        "# Train the model\n",
        "word2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Extract the trained word embeddings\n",
        "trained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('word2vec_embeddings_hate.npy', trained_embeddings)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "4Fk8FPLl4I0U",
        "outputId": "a8b73fac-ce4a-42b0-a55c-2d7dbec0b81a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Prepare labels (1 for correct context, 0 for negative samples)\\nlabels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\\n\\n# Train the model\\nword2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\\n\\n# Extract the trained word embeddings\\ntrained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\\n\\n# Save the embeddings\\nnp.save('word2vec_embeddings_hate.npy', trained_embeddings)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo83GuOqrVoS"
      },
      "outputs": [],
      "source": [
        "# Load the custom-trained Word2Vec embeddings\n",
        "# trained_embeddings = np.load('word2vec_embeddings_hate.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "towm2lPSrVoS"
      },
      "outputs": [],
      "source": [
        "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
        "labels = hate_train_data['Tag'].values\n",
        "\n",
        "# Now perform train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.1, random_state=60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YdSO8ka-rVoT"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data\n",
        "test_texts = hate_test_data['Sentence'].values\n",
        "test_texts_preprocessed = [preprocess_text(text) for text in test_texts]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining custom evalutation metric**"
      ],
      "metadata": {
        "id": "P4e4YGtX6aKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Jgl4FvHMrVoT"
      },
      "outputs": [],
      "source": [
        "# Define a custom macro F1 score function\n",
        "def macro_f1_score(y_true, y_pred):\n",
        "    # Ensure both y_true and y_pred are float32\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Convert predictions to binary (0 or 1)\n",
        "    y_pred_bin = tf.round(y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    def f1(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "        f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "        return f1_val\n",
        "\n",
        "    f1_per_class = f1(y_true, y_pred_bin)\n",
        "    return K.mean(f1_per_class)  # Macro F1 score (mean across all classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "xG1FK0v96hW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hBBspwjRrVoU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "0a77c528-4395-4562-d502-19e0cc1588dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Function to build the FFNN model for tuning\\ndef build_model(hp):\\n    model = Sequential()\\n\\n    embedding_dim = 100\\n    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\\n\\n    embedding_layer = Embedding(\\n        input_dim=vocab_size,\\n        output_dim=embedding_dim,\\n        input_length=sequence_len,\\n        weights=[trained_embeddings],\\n        trainable=False\\n    )\\n\\n    model.add(embedding_layer)\\n    model.add(Flatten())\\n\\n    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\\n    for i in range(num_layers):\\n        model.add(Dense(\\n            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\\n            activation='tanh'\\n        ))\\n\\n    # Binary classification\\n    model.add(Dense(1, activation='sigmoid'))\\n\\n    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\\n\\n    if optimizer_choice == 'Adam':\\n        opt = Adam(learning_rate=0.001)\\n    elif optimizer_choice == 'AdamW':\\n        opt = AdamW(learning_rate=0.001)\\n    else:\\n        opt = SGD(learning_rate=0.001)\\n\\n    model.compile(\\n        optimizer=opt,\\n        loss=BinaryCrossentropy(),\\n        metrics=[macro_f1_score]\\n    )\\n\\n    return model\\n\\n# Define the objective using KerasTuner's Objective class\\nobjective = Objective('val_macro_f1_score', direction='max')\\n\\n# Hyperparameter search with Keras Tuner\\ntuner = RandomSearch(\\n    build_model,\\n    objective=objective,  # Explicitly specify the objective\\n    max_trials=15,  # Number of hyperparameter configurations to try\\n    executions_per_trial=1,  # Number of times to train each model configuration\\n    directory='hyperparam_tuning_hate',\\n    project_name='ffnn_tuning_v7.2213123123'\\n)\\n\\n# Run the tuner search\\ntuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\\n\\n# Get the best model\\nbest_model_ffnn = tuner.get_best_models(num_models=1)[0]\\n\\n# Summary of the best model\\nbest_model_ffnn.summary()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''# Function to build the FFNN model for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=sequence_len,\n",
        "        weights=[trained_embeddings],\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\n",
        "    for i in range(num_layers):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\n",
        "            activation='tanh'\n",
        "        ))\n",
        "\n",
        "    # Binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "\n",
        "    if optimizer_choice == 'Adam':\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "    elif optimizer_choice == 'AdamW':\n",
        "        opt = AdamW(learning_rate=0.001)\n",
        "    else:\n",
        "        opt = SGD(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[macro_f1_score]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the objective using KerasTuner's Objective class\n",
        "objective = Objective('val_macro_f1_score', direction='max')\n",
        "\n",
        "# Hyperparameter search with Keras Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=objective,  # Explicitly specify the objective\n",
        "    max_trials=15,  # Number of hyperparameter configurations to try\n",
        "    executions_per_trial=1,  # Number of times to train each model configuration\n",
        "    directory='hyperparam_tuning_hate',\n",
        "    project_name='ffnn_tuning_v7.2213123123'\n",
        ")\n",
        "\n",
        "# Run the tuner search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model_ffnn = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model_ffnn.summary()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "E7-2_e8I6qCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IvRrrAXzrVoV"
      },
      "outputs": [],
      "source": [
        "# Save FFNN Model\n",
        "# best_model_ffnn.save('best_model_ffnn_hate.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Ujjwal21293/DSE-318/raw/refs/heads/main/Assignment2/best_model_ffnn_hate.keras"
      ],
      "metadata": {
        "id": "OyUOANloxSSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a27604-ac59-4a77-8e3a-3fac785796af"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 15:48:16--  https://github.com/Ujjwal21293/DSE-318/raw/refs/heads/main/Assignment2/best_model_ffnn_hate.keras\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Ujjwal21293/DSE-318/refs/heads/main/Assignment2/best_model_ffnn_hate.keras [following]\n",
            "--2025-04-04 15:48:17--  https://raw.githubusercontent.com/Ujjwal21293/DSE-318/refs/heads/main/Assignment2/best_model_ffnn_hate.keras\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8586315 (8.2M) [application/octet-stream]\n",
            "Saving to: ‘best_model_ffnn_hate.keras’\n",
            "\n",
            "best_model_ffnn_hat 100%[===================>]   8.19M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-04-04 15:48:18 (121 MB/s) - ‘best_model_ffnn_hate.keras’ saved [8586315/8586315]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the saved models**"
      ],
      "metadata": {
        "id": "X9OFtbJn6xR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved files\n",
        "best_model_ffnn_hate = tf.keras.models.load_model('best_model_ffnn_hate.keras', custom_objects={'macro_f1_score': macro_f1_score})"
      ],
      "metadata": {
        "id": "btp5estysaLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5748daa-1dcb-40a5-c411-c68b6b3c92ff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vW0aeQXwrVoV"
      },
      "outputs": [],
      "source": [
        "# Function to extract and print the macro avg F1-score\n",
        "def print_macro_f1(classification_report_dict, model_name):\n",
        "    macro_f1 = classification_report_dict['macro avg']['f1-score']\n",
        "    print(f\"{model_name} Model Macro Average F1-score: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "BFk0sGAZ66oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qGzqtOPnrVoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a749c1-8663-46d5-b025-fd03b1c66013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "<Sequential name=sequential, built=True> - Accuracy: 0.5689, Macro F1: 0.5592\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Binary labels (0 or 1)\n",
        "test_labels = hate_test_data['Tag'].values.reshape(-1, 1)\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "model = best_model_ffnn_hate\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "reports = {}\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Generate classification report\n",
        "report = classification_report(\n",
        "test_labels,\n",
        "predictions,\n",
        "target_names=['Non-Hate (0)', 'Hate (1)'],  # Optional: change labels as you wish\n",
        "output_dict=True)\n",
        "\n",
        "reports[model] = report\n",
        "\n",
        "# Print macro F1 score and optionally others\n",
        "f1 = f1_score(test_labels, predictions, average='macro')\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"{model} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCT3FEXdgerw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JY9Fy0Ylgelu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8MZz9Ib6u-Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Humor Dataset"
      ],
      "metadata": {
        "id": "sRKnKiRxF_Zg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aswkBEg5GEJi"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "hate_train_data = pd.read_csv('/content/train.csv.1')\n",
        "hate_test_data = pd.read_csv('/content/val.csv.1')\n",
        "\n",
        "# Prepare the text and labels\n",
        "texts = hate_train_data['Sentence'].values\n",
        "labels = hate_train_data['Tag'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing and Embedding**"
      ],
      "metadata": {
        "id": "c0RJyg0vGEJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LVclC5_3GEJj"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization and lemmatization using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Stopword removal and lemmatization\n",
        "    text = re.sub(r\"https\\\\S+|www\\\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\\\s]\", \"\", text)  # remove special characters\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "texts_preprocessed = [preprocess_text(text) for text in texts]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_preprocessed)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts_preprocessed)\n",
        "\n",
        "max_sequence_len = 128\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "n_VfMuGoGEJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "c9f78e5c-03db-482b-db12-56d9147082c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Preparing dataset\\nsentences = [text.split() for text in texts_preprocessed]\\nvocab_size = len(tokenizer.word_index) + 1\\n\\n# Define constants\\nembedding_dim = 100\\nwindow_size = 5  # For skip-gram context window\\n\\n# Build a simple skip-gram pair generator function\\ndef generate_skipgram_pairs(sentences, window_size, vocab_size):\\n    skipgrams = []\\n    for sentence in sentences:\\n        for i, word in enumerate(sentence):\\n            if word not in tokenizer.word_index:\\n                continue  # Skip unknown words\\n            target_word = tokenizer.word_index[word]\\n            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\\n            context_words = [\\n                tokenizer.word_index[w]\\n                for w in context_window\\n                if w != word and w in tokenizer.word_index\\n            ]\\n            for context_word in context_words:\\n                skipgrams.append([target_word, context_word])\\n    return np.array(skipgrams)\\n\\nskipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "'''# Preparing dataset\n",
        "sentences = [text.split() for text in texts_preprocessed]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define constants\n",
        "embedding_dim = 100\n",
        "window_size = 5  # For skip-gram context window\n",
        "\n",
        "# Build a simple skip-gram pair generator function\n",
        "def generate_skipgram_pairs(sentences, window_size, vocab_size):\n",
        "    skipgrams = []\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in tokenizer.word_index:\n",
        "                continue  # Skip unknown words\n",
        "            target_word = tokenizer.word_index[word]\n",
        "            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\n",
        "            context_words = [\n",
        "                tokenizer.word_index[w]\n",
        "                for w in context_window\n",
        "                if w != word and w in tokenizer.word_index\n",
        "            ]\n",
        "            for context_word in context_words:\n",
        "                skipgrams.append([target_word, context_word])\n",
        "    return np.array(skipgrams)\n",
        "\n",
        "skipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Splitting target words and context words\n",
        "X_target, X_context = zip(*skipgrams)\n",
        "X_target = np.array(X_target)\n",
        "X_context = np.array(X_context)\n",
        "\n",
        "# Defining the custom Word2Vec model using Keras\n",
        "input_target = tf.keras.layers.Input(shape=(1,))\n",
        "input_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "# Embedding layer for target and context\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\n",
        "target_embedding = embedding(input_target)\n",
        "context_embedding = embedding(input_context)'''"
      ],
      "metadata": {
        "id": "mkmcgNMGGEJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d2946b7a-6501-42b8-add7-6baf53ac1ad5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Splitting target words and context words\\nX_target, X_context = zip(*skipgrams)\\nX_target = np.array(X_target)\\nX_context = np.array(X_context)\\n\\n# Defining the custom Word2Vec model using Keras\\ninput_target = tf.keras.layers.Input(shape=(1,))\\ninput_context = tf.keras.layers.Input(shape=(1,))\\n\\n# Embedding layer for target and context\\nembedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\\ntarget_embedding = embedding(input_target)\\ncontext_embedding = embedding(input_context)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Reshape embedding output for dot product calculation\n",
        "target_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\n",
        "context_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\n",
        "\n",
        "# Compute dot product (cosine similarity between target and context)\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# Define the model and compile it\n",
        "word2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
        "word2vec_model.compile(optimizer='adam', loss='binary_crossentropy')'''"
      ],
      "metadata": {
        "id": "L80pHT2yGEJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "63a1d0f7-d0d7-46e7-93ad-88ddf30512a1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Reshape embedding output for dot product calculation\\ntarget_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\\ncontext_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\\n\\n# Compute dot product (cosine similarity between target and context)\\ndot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\\n\\n# Define the model and compile it\\nword2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\\nword2vec_model.compile(optimizer='adam', loss='binary_crossentropy')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Prepare labels (1 for correct context, 0 for negative samples)\n",
        "labels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\n",
        "\n",
        "# Train the model\n",
        "word2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Extract the trained word embeddings\n",
        "trained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('word2vec_embeddings_humor.npy', trained_embeddings)'''"
      ],
      "metadata": {
        "id": "6F6bsvFDGEJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "342ee168-fd06-498a-e92b-c14c50d24541"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Prepare labels (1 for correct context, 0 for negative samples)\\nlabels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\\n\\n# Train the model\\nword2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\\n\\n# Extract the trained word embeddings\\ntrained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\\n\\n# Save the embeddings\\nnp.save('word2vec_embeddings_humor.npy', trained_embeddings)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YcB8j542GEJn"
      },
      "outputs": [],
      "source": [
        "# Load the custom-trained Word2Vec embeddings\n",
        "# trained_embeddings = np.load('word2vec_embeddings_humor.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OmAM6bwiGEJn"
      },
      "outputs": [],
      "source": [
        "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
        "labels = hate_train_data['Tag'].values\n",
        "\n",
        "# Now perform train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.1, random_state=60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hACm0p6QGEJn"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data\n",
        "test_texts = hate_test_data['Sentence'].values\n",
        "test_texts_preprocessed = [preprocess_text(text) for text in test_texts]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "e3TWLQhzGEJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9j5RYc2YGEJo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "99ec6577-d108-456d-d4be-9eaf5c5a1dd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Function to build the FFNN model for tuning\\ndef build_model(hp):\\n    model = Sequential()\\n\\n    embedding_dim = 100\\n    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\\n\\n    embedding_layer = Embedding(\\n        input_dim=vocab_size,\\n        output_dim=embedding_dim,\\n        input_length=sequence_len,\\n        weights=[trained_embeddings],\\n        trainable=False\\n    )\\n\\n    model.add(embedding_layer)\\n    model.add(Flatten())\\n\\n    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\\n    for i in range(num_layers):\\n        model.add(Dense(\\n            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\\n            activation='tanh'\\n        ))\\n\\n    # Binary classification\\n    model.add(Dense(1, activation='sigmoid'))\\n\\n    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\\n\\n    if optimizer_choice == 'Adam':\\n        opt = Adam(learning_rate=0.001)\\n    elif optimizer_choice == 'AdamW':\\n        opt = AdamW(learning_rate=0.001)\\n    else:\\n        opt = SGD(learning_rate=0.001)\\n\\n    model.compile(\\n        optimizer=opt,\\n        loss=BinaryCrossentropy(),\\n        metrics=[macro_f1_score]\\n    )\\n\\n    return model\\n\\n# Define the objective using KerasTuner's Objective class\\nobjective = Objective('val_macro_f1_score', direction='max')\\n\\n# Hyperparameter search with Keras Tuner\\ntuner = RandomSearch(\\n    build_model,\\n    objective=objective,  # Explicitly specify the objective\\n    max_trials=15,  # Number of hyperparameter configurations to try\\n    executions_per_trial=1,  # Number of times to train each model configuration\\n    directory='hyperparam_tuning_humor',\\n    project_name='ffnn_tuning_v7.2213123123'\\n)\\n\\n# Run the tuner search\\ntuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\\n\\n# Get the best model\\nbest_model_ffnn = tuner.get_best_models(num_models=1)[0]\\n\\n# Summary of the best model\\nbest_model_ffnn.summary()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "'''# Function to build the FFNN model for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=sequence_len,\n",
        "        weights=[trained_embeddings],\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\n",
        "    for i in range(num_layers):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\n",
        "            activation='tanh'\n",
        "        ))\n",
        "\n",
        "    # Binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "\n",
        "    if optimizer_choice == 'Adam':\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "    elif optimizer_choice == 'AdamW':\n",
        "        opt = AdamW(learning_rate=0.001)\n",
        "    else:\n",
        "        opt = SGD(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[macro_f1_score]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the objective using KerasTuner's Objective class\n",
        "objective = Objective('val_macro_f1_score', direction='max')\n",
        "\n",
        "# Hyperparameter search with Keras Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=objective,  # Explicitly specify the objective\n",
        "    max_trials=15,  # Number of hyperparameter configurations to try\n",
        "    executions_per_trial=1,  # Number of times to train each model configuration\n",
        "    directory='hyperparam_tuning_humor',\n",
        "    project_name='ffnn_tuning_v7.2213123123'\n",
        ")\n",
        "\n",
        "# Run the tuner search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model_ffnn = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model_ffnn.summary()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "na_P4mPsGEJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "41HcVShwGEJp"
      },
      "outputs": [],
      "source": [
        "# Save FFNN Model\n",
        "# best_model_ffnn.save('best_model_ffnn_humor.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Ujjwal21293/DSE-318/raw/refs/heads/main/Assignment2/best_model_ffnn_humor.keras"
      ],
      "metadata": {
        "id": "-B778cpLx-A-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99191728-8b5d-4e7e-94c2-8668d7b8f504"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 15:50:56--  https://github.com/Ujjwal21293/DSE-318/raw/refs/heads/main/Assignment2/best_model_ffnn_humor.keras\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Ujjwal21293/DSE-318/refs/heads/main/Assignment2/best_model_ffnn_humor.keras [following]\n",
            "--2025-04-04 15:50:57--  https://raw.githubusercontent.com/Ujjwal21293/DSE-318/refs/heads/main/Assignment2/best_model_ffnn_humor.keras\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5316506 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘best_model_ffnn_humor.keras’\n",
            "\n",
            "best_model_ffnn_hum 100%[===================>]   5.07M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-04 15:50:57 (87.1 MB/s) - ‘best_model_ffnn_humor.keras’ saved [5316506/5316506]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the saved models**"
      ],
      "metadata": {
        "id": "JItgJIIdGEJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved files\n",
        "best_model_ffnn = tf.keras.models.load_model('best_model_ffnn_humor.keras', custom_objects={'macro_f1_score': macro_f1_score})"
      ],
      "metadata": {
        "id": "pHctoPgcGEJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0d4f29-56ee-4d23-dfa8-761558b782d8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ImnPC3Z3GEJp"
      },
      "outputs": [],
      "source": [
        "# Function to extract and print the macro avg F1-score\n",
        "def print_macro_f1(classification_report_dict, model_name):\n",
        "    macro_f1 = classification_report_dict['macro avg']['f1-score']\n",
        "    print(f\"{model_name} Model Macro Average F1-score: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "7vbn8YCRGEJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "TmzRBX1TGEJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086ede91-67f0-4ae1-fd2c-2657b98587f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "<Sequential name=sequential, built=True> - Accuracy: 0.5966, Macro F1: 0.3737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Binary labels (0 or 1)\n",
        "test_labels = hate_test_data['Tag'].values.reshape(-1, 1)\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "model = best_model_ffnn\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "reports = {}\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Generate classification report\n",
        "report = classification_report(\n",
        "test_labels,\n",
        "predictions,\n",
        "target_names=['Non-Hate (0)', 'Hate (1)'],  # Optional: change labels as you wish\n",
        "output_dict=True)\n",
        "\n",
        "reports[model] = report\n",
        "\n",
        "# Print macro F1 score and optionally others\n",
        "f1 = f1_score(test_labels, predictions, average='macro')\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"{model} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BH7c3dHLgb8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjtii9pPgb0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jc2eDTg_GRwx"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarcasm Dataset"
      ],
      "metadata": {
        "id": "vCxf1YIJGTZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EozSQAW3GlUu"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "hate_train_data = pd.read_csv('/content/train.csv.2')\n",
        "hate_test_data = pd.read_csv('/content/val.csv.2')\n",
        "\n",
        "# Prepare the text and labels\n",
        "texts = hate_train_data['Sentence'].values\n",
        "labels = hate_train_data['Tag'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing and Embedding**"
      ],
      "metadata": {
        "id": "-uFkLbwNGlUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "GJxJ1KH9GlUw"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization and lemmatization using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Stopword removal and lemmatization\n",
        "    text = re.sub(r\"https\\\\S+|www\\\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\\\s]\", \"\", text)  # remove special characters\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "texts_preprocessed = [preprocess_text(text) for text in texts]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_preprocessed)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts_preprocessed)\n",
        "\n",
        "max_sequence_len = 128\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "BnOzwBDmGlUw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "2eb555a2-36be-4f74-b533-1a840db2c6e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Preparing dataset\\nsentences = [text.split() for text in texts_preprocessed]\\nvocab_size = len(tokenizer.word_index) + 1\\n\\n# Define constants\\nembedding_dim = 100\\nwindow_size = 5  # For skip-gram context window\\n\\n# Build a simple skip-gram pair generator function\\ndef generate_skipgram_pairs(sentences, window_size, vocab_size):\\n    skipgrams = []\\n    for sentence in sentences:\\n        for i, word in enumerate(sentence):\\n            if word not in tokenizer.word_index:\\n                continue  # Skip unknown words\\n            target_word = tokenizer.word_index[word]\\n            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\\n            context_words = [\\n                tokenizer.word_index[w]\\n                for w in context_window\\n                if w != word and w in tokenizer.word_index\\n            ]\\n            for context_word in context_words:\\n                skipgrams.append([target_word, context_word])\\n    return np.array(skipgrams)\\n\\nskipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "'''# Preparing dataset\n",
        "sentences = [text.split() for text in texts_preprocessed]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define constants\n",
        "embedding_dim = 100\n",
        "window_size = 5  # For skip-gram context window\n",
        "\n",
        "# Build a simple skip-gram pair generator function\n",
        "def generate_skipgram_pairs(sentences, window_size, vocab_size):\n",
        "    skipgrams = []\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in tokenizer.word_index:\n",
        "                continue  # Skip unknown words\n",
        "            target_word = tokenizer.word_index[word]\n",
        "            context_window = sentence[max(i - window_size, 0): min(i + window_size + 1, len(sentence))]\n",
        "            context_words = [\n",
        "                tokenizer.word_index[w]\n",
        "                for w in context_window\n",
        "                if w != word and w in tokenizer.word_index\n",
        "            ]\n",
        "            for context_word in context_words:\n",
        "                skipgrams.append([target_word, context_word])\n",
        "    return np.array(skipgrams)\n",
        "\n",
        "skipgrams = generate_skipgram_pairs(sentences, window_size, vocab_size)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Splitting target words and context words\n",
        "X_target, X_context = zip(*skipgrams)\n",
        "X_target = np.array(X_target)\n",
        "X_context = np.array(X_context)\n",
        "\n",
        "# Defining the custom Word2Vec model using Keras\n",
        "input_target = tf.keras.layers.Input(shape=(1,))\n",
        "input_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "# Embedding layer for target and context\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\n",
        "target_embedding = embedding(input_target)\n",
        "context_embedding = embedding(input_context)'''"
      ],
      "metadata": {
        "id": "kFZrgK9fGlUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c367ed03-7d9a-49bf-f593-f493c91eb509"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Splitting target words and context words\\nX_target, X_context = zip(*skipgrams)\\nX_target = np.array(X_target)\\nX_context = np.array(X_context)\\n\\n# Defining the custom Word2Vec model using Keras\\ninput_target = tf.keras.layers.Input(shape=(1,))\\ninput_context = tf.keras.layers.Input(shape=(1,))\\n\\n# Embedding layer for target and context\\nembedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\")\\ntarget_embedding = embedding(input_target)\\ncontext_embedding = embedding(input_context)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Reshape embedding output for dot product calculation\n",
        "target_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\n",
        "context_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\n",
        "\n",
        "# Compute dot product (cosine similarity between target and context)\n",
        "dot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# Define the model and compile it\n",
        "word2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
        "word2vec_model.compile(optimizer='adam', loss='binary_crossentropy')'''"
      ],
      "metadata": {
        "id": "gHWj4vJ8GlUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "5b4903af-d6ec-453d-a549-ece825c029ca"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Reshape embedding output for dot product calculation\\ntarget_embedding = tf.keras.layers.Reshape((embedding_dim,))(target_embedding)\\ncontext_embedding = tf.keras.layers.Reshape((embedding_dim,))(context_embedding)\\n\\n# Compute dot product (cosine similarity between target and context)\\ndot_product = tf.keras.layers.Dot(axes=1)([target_embedding, context_embedding])\\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dot_product)\\n\\n# Define the model and compile it\\nword2vec_model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\\nword2vec_model.compile(optimizer='adam', loss='binary_crossentropy')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Prepare labels (1 for correct context, 0 for negative samples)\n",
        "labels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\n",
        "\n",
        "# Train the model\n",
        "word2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Extract the trained word embeddings\n",
        "trained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('word2vec_embeddings_sarcasm.npy', trained_embeddings)'''"
      ],
      "metadata": {
        "id": "DYXO0uaoGlUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0b824c53-beeb-4f59-9036-9685bb77edfd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Prepare labels (1 for correct context, 0 for negative samples)\\nlabels = np.ones((len(skipgrams), 1))  # Positive samples are labeled 1\\n\\n# Train the model\\nword2vec_model.fit([X_target, X_context], labels, epochs=5, batch_size=128)\\n\\n# Extract the trained word embeddings\\ntrained_embeddings = word2vec_model.get_layer('embedding').get_weights()[0]\\n\\n# Save the embeddings\\nnp.save('word2vec_embeddings_sarcasm.npy', trained_embeddings)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fY62Cd4OGlUy"
      },
      "outputs": [],
      "source": [
        "# Load the custom-trained Word2Vec embeddings\n",
        "# trained_embeddings = np.load('word2vec_embeddings_sarcasm.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AUbjVvd7GlUy"
      },
      "outputs": [],
      "source": [
        "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
        "labels = hate_train_data['Tag'].values\n",
        "\n",
        "# Now perform train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.1, random_state=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "AjpZCGi-GlUz"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test data\n",
        "test_texts = hate_test_data['Sentence'].values\n",
        "test_texts_preprocessed = [preprocess_text(text) for text in test_texts]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "Wn5ItCOrGlUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "aPPjXpXxGlU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d40c14da-6dd7-426b-a768-a4c38bbbd6f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Function to build the FFNN model for tuning\\ndef build_model(hp):\\n    model = Sequential()\\n\\n    embedding_dim = 100\\n    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\\n\\n    embedding_layer = Embedding(\\n        input_dim=vocab_size,\\n        output_dim=embedding_dim,\\n        input_length=sequence_len,\\n        weights=[trained_embeddings],\\n        trainable=False\\n    )\\n\\n    model.add(embedding_layer)\\n    model.add(Flatten())\\n\\n    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\\n    for i in range(num_layers):\\n        model.add(Dense(\\n            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\\n            activation='tanh'\\n        ))\\n\\n    # Binary classification\\n    model.add(Dense(1, activation='sigmoid'))\\n\\n    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\\n\\n    if optimizer_choice == 'Adam':\\n        opt = Adam(learning_rate=0.001)\\n    elif optimizer_choice == 'AdamW':\\n        opt = AdamW(learning_rate=0.001)\\n    else:\\n        opt = SGD(learning_rate=0.001)\\n\\n    model.compile(\\n        optimizer=opt,\\n        loss=BinaryCrossentropy(),\\n        metrics=[macro_f1_score]\\n    )\\n\\n    return model\\n\\n# Define the objective using KerasTuner's Objective class\\nobjective = Objective('val_macro_f1_score', direction='max')\\n\\n# Hyperparameter search with Keras Tuner\\ntuner = RandomSearch(\\n    build_model,\\n    objective=objective,  # Explicitly specify the objective\\n    max_trials=15,  # Number of hyperparameter configurations to try\\n    executions_per_trial=1,  # Number of times to train each model configuration\\n    directory='hyperparam_tuning_sarcasm',\\n    project_name='ffnn_tuning_v7.2213123123'\\n)\\n\\n# Run the tuner search\\ntuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\\n\\n# Get the best model\\nbest_model_ffnn = tuner.get_best_models(num_models=1)[0]\\n\\n# Summary of the best model\\nbest_model_ffnn.summary()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "'''# Function to build the FFNN model for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    sequence_len = hp.Int('sequence_len', min_value=64, max_value=128, step=16)\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=sequence_len,\n",
        "        weights=[trained_embeddings],\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Flatten())\n",
        "\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=4)\n",
        "    for i in range(num_layers):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=64, step=16),\n",
        "            activation='tanh'\n",
        "        ))\n",
        "\n",
        "    # Binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimizer_choice = hp.Choice('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "\n",
        "    if optimizer_choice == 'Adam':\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "    elif optimizer_choice == 'AdamW':\n",
        "        opt = AdamW(learning_rate=0.001)\n",
        "    else:\n",
        "        opt = SGD(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=[macro_f1_score]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the objective using KerasTuner's Objective class\n",
        "objective = Objective('val_macro_f1_score', direction='max')\n",
        "\n",
        "# Hyperparameter search with Keras Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=objective,  # Explicitly specify the objective\n",
        "    max_trials=15,  # Number of hyperparameter configurations to try\n",
        "    executions_per_trial=1,  # Number of times to train each model configuration\n",
        "    directory='hyperparam_tuning_sarcasm',\n",
        "    project_name='ffnn_tuning_v7.2213123123'\n",
        ")\n",
        "\n",
        "# Run the tuner search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model_ffnn = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Summary of the best model\n",
        "best_model_ffnn.summary()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "xe0kWgDEGlU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XBeWZ2D1GlU0"
      },
      "outputs": [],
      "source": [
        "# Save FFNN Model\n",
        "# best_model_ffnn.save('best_model_ffnn_sarcasm.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Ujjwal21293/DSE-318/raw/refs/heads/main/Assignment2/best_model_ffnn_sarcasm.keras"
      ],
      "metadata": {
        "id": "m39q0Ie_1_XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952cc243-31e9-40a3-f0a5-e5ed17011ef0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 15:52:38--  https://github.com/Ujjwal21293/DSE-318/raw/refs/heads/main/Assignment2/best_model_ffnn_sarcasm.keras\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Ujjwal21293/DSE-318/refs/heads/main/Assignment2/best_model_ffnn_sarcasm.keras [following]\n",
            "--2025-04-04 15:52:38--  https://raw.githubusercontent.com/Ujjwal21293/DSE-318/refs/heads/main/Assignment2/best_model_ffnn_sarcasm.keras\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8319259 (7.9M) [application/octet-stream]\n",
            "Saving to: ‘best_model_ffnn_sarcasm.keras’\n",
            "\n",
            "best_model_ffnn_sar 100%[===================>]   7.93M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-04 15:52:39 (126 MB/s) - ‘best_model_ffnn_sarcasm.keras’ saved [8319259/8319259]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the saved models**"
      ],
      "metadata": {
        "id": "dPjsF4HyGlU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved files\n",
        "best_model_ffnn = tf.keras.models.load_model('best_model_ffnn_sarcasm.keras', custom_objects={'macro_f1_score': macro_f1_score})"
      ],
      "metadata": {
        "id": "MN0YloI3GlU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27b7d46-f7e9-4d25-de46-4e1e42c96312"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 18 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VjXOn6OsGlU1"
      },
      "outputs": [],
      "source": [
        "# Function to extract and print the macro avg F1-score\n",
        "def print_macro_f1(classification_report_dict, model_name):\n",
        "    macro_f1 = classification_report_dict['macro avg']['f1-score']\n",
        "    print(f\"{model_name} Model Macro Average F1-score: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "76NwrGrXGlU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "iS5Tsf48GlU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68fd019-84a8-4aab-ca22-2fc1c7a6e36f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "<Sequential name=sequential, built=True> - Accuracy: 0.8419, Macro F1: 0.7073\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# Binary labels (0 or 1)\n",
        "test_labels = hate_test_data['Tag'].values.reshape(-1, 1)\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "model = best_model_ffnn\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "reports = {}\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Generate classification report\n",
        "report = classification_report(\n",
        "test_labels,\n",
        "predictions,\n",
        "target_names=['Non-Hate (0)', 'Hate (1)'],  # Optional: change labels as you wish\n",
        "output_dict=True)\n",
        "\n",
        "reports[model] = report\n",
        "\n",
        "# Print macro F1 score and optionally others\n",
        "f1 = f1_score(test_labels, predictions, average='macro')\n",
        "acc = accuracy_score(test_labels, predictions)\n",
        "print(f\"{model} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1Hf186ZVKE_"
      },
      "execution_count": 64,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}